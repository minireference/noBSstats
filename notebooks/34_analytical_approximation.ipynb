{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73f3d18-88c7-499d-90a3-dd6954247cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 3.4 — Analytical approximation methods\n",
    "\n",
    "This notebook contains the code examples from [Section 3.4 Analytical approximation methods]() of the **No Bullshit Guide to Statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce8e91-8824-4103-a526-02e23c794668",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990de07d-c26f-4a29-935a-2b1dacc89b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3209c5be-cb6a-42c0-8bf8-949a4fc0ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures setup\n",
    "sns.set_theme(\n",
    "    context=\"paper\",\n",
    "    style=\"whitegrid\",\n",
    "    palette=\"colorblind\",\n",
    "    rc={'figure.figsize': (7,4)},\n",
    ")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df22d8d7-6814-4c3b-a3d3-2f31ab6b2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for repeatability\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad416305-607b-48ca-9036-afc85ac05ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c0804c-6b22-4b48-97bb-13506b800e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for two groups\n",
    "data = pd.read_csv('../stats_overview/data/employee_lifetime_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0b82b-f09f-4ddf-b68c-e0a8b83de125",
   "metadata": {},
   "source": [
    "## Approach 2: Analytical approximations for hypothesis testing\n",
    "\n",
    "We'll now look at another approach for answering Question 1:\n",
    "using and analytical approximation,\n",
    "which is the way normally taught in STATS 101 courses.\n",
    "How likely or unlikely is the observed difference $d=130$ under the null hypothesis?\n",
    "\n",
    "- Analytical approximations are math models for describing the sampling distribution under $H_0$\n",
    "  - Real sampling distributions: obtained by repeated sampling from $H_0$\n",
    "  - Analytical approximation: probability model based on estimated parameters\n",
    "- Based on this assumption we can use the theoretical model for the difference between group means\n",
    "  that we developed earlier, we can obtain a **closed form expression** for the sampling distribution of $D$.\n",
    "- In particular, the probability model for the two groups under $H_0$ are:\n",
    "  $$ \n",
    "       H_0: \\qquad X_S = \\mathcal{N}(\\color{red}{\\mu_0}, \\sigma_S)\n",
    "       \\quad \\textrm{and} \\quad\n",
    "       X_{NS} = \\mathcal{N}(\\color{red}{\\mu_0}, \\sigma_{NS}), \\quad\n",
    "  $$\n",
    "  from which we can derive the model for $D = \\overline{\\mathbf{X}}_S - \\overline{\\mathbf{X}}_{NS}$:\n",
    "  $$ \n",
    "     D  \\sim \\mathcal{N}\\!\\left( \\color{red}{0}, \\  \\sqrt{ \\tfrac{\\sigma^2_S}{n_S} + \\tfrac{\\sigma^2_{NS}}{n_{NS}} } \\right)\n",
    "  $$\n",
    "  In words, the sampling distribution of the difference between group means is\n",
    "  normally distributed with mean $\\mu_D = 0$ and standard deviation $\\sigma_D$,\n",
    "  which depends on the variance of the two groups $\\sigma^2_S$ and $\\sigma^2_{NS}$.\n",
    "  Recall we obtained this expression earlier when we discussed difference of means between groups A and B.\n",
    "- However, the population variances $\\sigma^2_S$ and $\\sigma^2_{NS}$ are unknown,\n",
    "  and we only have the estimated variances $s_S^2$ and $s_{NS}^2$,\n",
    "  which we calculated from the sample.\n",
    "- That's OK though, since the sample variances are estimates for the population variances.\n",
    "  There are two common ways to obtain an approximation for $\\sigma^2_D$:\n",
    "  - Pooled variance: $\\sigma^2_D \\approx s^2_p =  \\frac{(n_S-1)s_S^2 \\; + \\; (n_{NS}-1)s_{NS}^2}{n_S + n_{NS} - 2}$\n",
    "    (takes advantage of assumption that both samples come from the same population under $H_0$)\n",
    "  - Unpooled variance: $\\sigma^2_D \\approx s^2_u = \\tfrac{s^2_S}{n_S} + \\tfrac{s^2_{NS}}{n_{NS}}$\n",
    "    (follows from the general rule of probability theory)\n",
    "- NEW CONCEPT: **Student's $t$-distribution** is a model for $D$ which takes into account\n",
    "  we are using $s_S^2$ and $s_{NS}^2$ instead of $\\sigma_S^2$ and $\\sigma_{NS}^2$.\n",
    "- NEW CONCEPT: **degrees of freedom**, denoted `df` in code or $\\nu$ (Greek letter *nu*) in equations,\n",
    "  is the parameter of Student's $t$-distribution dependent on the sample size used to estimate quantities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96fbdb-9ff0-4f09-b2b1-8a5bb4a027b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Student's t-test (pooled variance)\n",
    "\n",
    "[Student's $t$-test for comparison of the difference between two groups means](https://statkat.com/stattest.php?&t=9) is a procedure that makes use of the pooled variance $s^2_p$. \n",
    "Recall $H_0$ states there is no difference between the two groups.\n",
    "This means we can think of $s_S^2$ and $s_{NS}^2$ as two independent estimates of the population variance,\n",
    "so we can combine them (pool them together) to obtain an estimate $s^2_p$.\n",
    "\n",
    "#### Black-box approach\n",
    "The `scipy.stats` function `ttest_ind` will perform all the steps of Student's $t$-test procedure,\n",
    "without the need for us to understand the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12027307-a10e-42f7-8c89-ee05a2e1be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046999086677830995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# extract data for two groups\n",
    "xS = data[data[\"group\"]==\"S\"]['ELV']\n",
    "xNS = data[data[\"group\"]==\"NS\"]['ELV']\n",
    "\n",
    "# run the complete t-test procedure for ind-ependent samples:\n",
    "result = ttest_ind(xS, xNS)\n",
    "result.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd780cf-c255-4bb3-8a73-a703dd535227",
   "metadata": {},
   "source": [
    "The $p$-value is less than 0.05 so our decision is to **reject the null hypothesis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a96f3-acc6-4662-83db-8f1f9bc4d9f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Student's t-test under the hood\n",
    "\n",
    "The computations hidden behind the function `ttest_ind` involve a six step procedure that makes use of the pooled variance $s^2_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f046a3e0-b61e-4e4b-9342-b343320e386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04699908667783097"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import stdev\n",
    "from scipy.stats import t as tdist\n",
    "\n",
    "# 1. calculate the mean in each group\n",
    "meanS, meanNS = np.mean(xS), np.mean(xNS)\n",
    "\n",
    "# 2. calculate d, the observed difference between means\n",
    "d = meanS - meanNS\n",
    "\n",
    "# 3. calculate the standard deviations in each group\n",
    "stdS, stdNS = stdev(xS), stdev(xNS)\n",
    "nS, nNS = len(xS), len(xNS)\n",
    "\n",
    "# 4. compute the pooled variance and standard error\n",
    "var_pooled = ((nS-1)*stdS**2 + (nNS-1)*stdNS**2)/(nS + nNS - 2)\n",
    "std_pooled = np.sqrt(var_pooled)\n",
    "std_err = std_pooled * np.sqrt(1/nS + 1/nNS)\n",
    "\n",
    "# 5. compute the value of the t-statistic\n",
    "tstat = d / std_err\n",
    "\n",
    "# 6. obtain the p-value for the t-statistic from a \n",
    "#    t-distribution with 31+30-2 = 59 degrees of freedom\n",
    "df = nS + nNS - 2\n",
    "pvalue = 2 * tdist(df).cdf(-abs(tstat))  # 2* because two-sided\n",
    "\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750085e0-1b12-4857-ba8d-8c2169400958",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Welch's t-test (unpooled variances)\n",
    "\n",
    "An [alternative $t$-test procedure](https://statkat.com/stattest.php?&t=9) that doesn't assume the variances in the two groups are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180dd392-709e-42ee-ad4d-1e6a85b3f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046579019827041344"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = ttest_ind(xS, xNS, equal_var=False)\n",
    "result2.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249ec74-263d-4131-b118-226ba4e42af2",
   "metadata": {},
   "source": [
    "Welch's $t$-test differs only in steps 4 through 6 as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7f17ca-1c3a-4426-b8b4-a1318c6126df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04657901982704139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4'. compute the unpooled standard deviation of D\n",
    "stdD = np.sqrt(stdS**2/nS + stdNS**2/nNS)\n",
    "\n",
    "# 5'. compute the value of the t-statistic\n",
    "tstat = d / stdD\n",
    "\n",
    "# 6'. obtain the p-value from a t-distribution from\n",
    "# this crazy formula for the degrees of freedom:\n",
    "df = (stdS**2/nS + stdNS**2/nNS)**2 / \\\n",
    "    ((stdS**2/nS)**2/(nS-1) + (stdNS**2/nNS)**2/(nNS-1) )\n",
    "pvalue = 2 * tdist(df).cdf(-abs(tstat))  # 2* because two-sided\n",
    "\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38636dfd-5d89-48b8-bb17-9bd37908429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b995c886-ec75-4ec9-a44f-f1e337553ff1",
   "metadata": {},
   "source": [
    "### Summary of Question 1\n",
    "\n",
    "We saw two ways to answer Question 1 (is there really a difference between group means) and obtain the $p$-value.\n",
    "We interpreted the small $p$-values as evidence that the observed difference, $d=130$, is unlikely to be due to chance under $H_0$, so we rejected the null hypothesis.\n",
    "Note this whole procedure is just a sanity check—we haven't touched the alternative hypothesis at all yet,\n",
    "and for all we know the stats training could have the effect of decreasing ELV!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07e85c-f838-4a7d-9b5f-22de91e06c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a85eff-8ace-4454-a1b2-dec424ac2cc4",
   "metadata": {},
   "source": [
    "## Estimating the effect size\n",
    "\n",
    "- Question 2 of Amy's statistical investigation is to estimate the difference in ELV gained by stats training.\n",
    "- NEW CONCEPT: **effect size** is a measure of difference between intervention and control groups.\n",
    "- We assume the data of **Group S** and **Group NS** come from different populations with means $\\mu_S$ and $\\mu_{NS}$.\n",
    "- We're interested in estimating the difference between population means, denoted $\\Delta = \\mu_S - \\mu_{NS}$.\n",
    "- By analyzing the sample, we have obtained an estimate $d=130$ for the unknown $\\Delta$,\n",
    "  but we know our data contains lots of variability, so we know our estimate might be off.\n",
    "- We want an answer to Question 2 (What is the estimated difference between group means?)\n",
    "  that takes into account the variability of the data.\n",
    "- NEW CONCEPT: **confidence interval** is a way to describe a range of values for an estimate\n",
    "  that takes into account the variability of the data.\n",
    "- We want to provide an answer to Question 2 in the form of a confidence interval that tells\n",
    "  us a range of values where we believe the true value of $\\Delta$ falls.\n",
    "- Similar to how we showed two approaches for hypothesis testing,\n",
    "  we'll work on effect size estimation using two approaches: bootstrap estimation and analytical approximation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51526097-4dac-4260-82d9-00cbe2a816fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe434e3-fabe-4670-969f-2bb523699c38",
   "metadata": {},
   "source": [
    "### Approach 2: Confidence intervals using analytical approximations\n",
    "\n",
    "  - Assumption: the variance of the two populations is the same (or approximately equal)\n",
    "  - Using the theoretical model for the populations,\n",
    "    we can obtain a formula for CI of effect size $\\Delta$:\n",
    "    $$\n",
    "    \\textrm{CI}_{(1-\\alpha)}\n",
    "    = \\left[ d - t^*\\!\\cdot\\!\\sigma_D, \\, \n",
    "             d + t^*\\!\\cdot\\!\\sigma_D\n",
    "      \\right].\n",
    "    $$\n",
    "    The confidence interval is centred at $d$,\n",
    "    with width proportional to the standard deviation $\\sigma_D$.\n",
    "    The constant $t^*$ denotes the value of the inverse CDF of Student's $t$-distribution\n",
    "    with appropriate number of degrees of freedom `df` evaluated at $1-\\frac{\\alpha}{2}$.\n",
    "    For a 90% confidence interval, we choose $\\alpha=0.10$,\n",
    "    which gives $(1-\\frac{\\alpha}{2}) = 0.95$, $t^* = F_{T_{\\textrm{df}}}^{-1}\\left(0.95\\right)$.\n",
    "  - We can use the two different analytical approximations to obtain a formula for $\\sigma_D$\n",
    "    just as we did in the hypothesis testing:\n",
    "    - Pooled variance: $\\sigma^2_p =  \\frac{(n_S-1)s_S^2 + (n_{NS}-1)s_{NS}^2}{n_S + n_{NS} - 2}$,\n",
    "      and `df` = $n_S + n_{NS} -2$\n",
    "    - Unpooled variance: $\\sigma^2_u = \\tfrac{s^2_A}{n_A} + \\tfrac{s^2_B}{n_B}$, and `df` = [...](https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_unequal_variances_(sX1_%3E_2sX2_or_sX2_%3E_2sX1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127fc6b-e0da-4a12-8f1b-8f34d8a35294",
   "metadata": {},
   "source": [
    "#### Using pooled variance\n",
    "\n",
    "The calculations are similar to Student's t-test for hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59e0ff3-2d05-45f6-b199-f49dc57a1f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.925130302086643, 237.12228905275202]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "d = np.mean(xS) - np.mean(xNS)\n",
    "\n",
    "nS, nNS = len(xS), len(xNS)\n",
    "stdS, stdNS = stdev(xS), stdev(xNS)\n",
    "var_pooled = ((nS-1)*stdS**2 + (nNS-1)*stdNS**2)/(nS + nNS - 2)\n",
    "std_pooled = np.sqrt(var_pooled)\n",
    "std_err = std_pooled * np.sqrt(1/nS + 1/nNS)\n",
    "\n",
    "df = nS + nNS - 2\n",
    "\n",
    "# for 90% confidence interval, need 10% in tails\n",
    "alpha = 0.10\n",
    "\n",
    "# now use inverse-CDF of Students t-distribution\n",
    "tstar = abs(t(df).ppf(alpha/2))\n",
    "\n",
    "CI_tpooled = [d - tstar*std_err, d + tstar*std_err]\n",
    "CI_tpooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0872d91-3824-430b-af77-6450baade842",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using unpooled variance\n",
    "\n",
    "The calculations are similar to the Welch's t-test for hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8018f17-1b81-4a53-9626-299c56a13ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.14219839967336, 236.9052209551653]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.mean(xS) - np.mean(xNS)\n",
    "\n",
    "nS, nNS = len(xS), len(xNS)\n",
    "stdS, stdNS = stdev(xS), stdev(xNS)\n",
    "stdD = np.sqrt(stdS**2/nS + stdNS**2/nNS)\n",
    "\n",
    "df = (stdS**2/nS + stdNS**2/nNS)**2 / \\\n",
    "    ((stdS**2/nS)**2/(nS-1) + (stdNS**2/nNS)**2/(nNS-1) )\n",
    "\n",
    "# for 90% confidence interval, need 10% in tails\n",
    "alpha = 0.10\n",
    "\n",
    "# now use inverse-CDF of Students t-distribution\n",
    "tstar = abs(t(df).ppf(alpha/2))\n",
    "\n",
    "CI_tunpooled = [d - tstar*stdD, d + tstar*stdD]\n",
    "CI_tunpooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd454080-5649-4d4b-8422-8174695485f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec3453-1181-47e5-87e8-1f1ca0bb6565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc47997-3012-4bff-ae74-b36ccdd15b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d127352-f6e9-4f70-8300-0c3427b06949",
   "metadata": {},
   "source": [
    "\n",
    "## Comparison of resampling methods and analytical approximations\n",
    "\n",
    "In this notebook we saw two different approaches for doing statistical analysis: resampling methods and analytical approximations. This is a general pattern in statistics where there is not only one correct answer: multiple approaches to data analysis are valid, and you need to think about the specifics of each situation. We'll learn about both approaches in the book.\n",
    "\n",
    "Analytical approximations are currently taught in most stats courses (STAT 101). Historically, analytical approximations have been used more widely because they require only simple arithmetic calculations: statistic practitioners (scientists, engineers, etc.) simply need to compute sample statistics, plug them into a formula, and obtain a $p$-value. This convenience comes at the cost of numerous assumptions about the data distribution, which often don't hold in practice (e.g. assuming population is normal, when it isn't).\n",
    "\n",
    "In recent years, resampling methods like the permutation test and bootstrap estimation are becoming more popular, widely used in industry, and increasingly also taught at universities (see this [blog post](https://minireference.com/blog/fixing-the-introductory-statistics-curriculum/) about the *modern statistics* curriculum). **The main advantage so resampling methods is that they require less modelling assumptions.** Procedures like the permutation test can be applied broadly to any scenario where two groups are being compared, and don't require developing specific formulas for different cases. Resampling methods are easier to understand since the they are directly related to the sampling distribution, and there are no formulas to memorize.\n",
    "\n",
    "Understanding resampling methods requires some basic familiarity with programming, but the skills required are not advanced: knowledge of variables, expressions, and basic `for` loop is sufficient. If you were able to follow the code examples described above (see `resample_under_H0`, `permutation_test`, and `bootstrap_stat`), then you've already **seen all the code you will need for the entire book!**  I've prepared a [python tutorial](https://nobsstats.com/tutorials/python_tutorial.html) to make readers with no prior experience with Python will be able to quickly pick up the syntax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
